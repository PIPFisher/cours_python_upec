{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<div align=\"right\">Enseignant : Aric Wizenberg</div>\n",
    "<div align=\"right\">E-mail : icarwiz@yahoo.fr</div>\n",
    "<div align=\"right\">Année : 2018/2019</div><br><br><br>\n",
    "<div align=\"center\"><span style=\"font-family:Lucida Caligraphy;font-size:32px;color:darkgreen\">Master 2 MASERATI - Cours de Python</span></div><br><br>\n",
    "<div align=\"center\"><span style=\"font-family:Lucida Caligraphy;font-size:24px;color:#e60000\">Le framework Scrapy et Selenium</span></div><br><br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les XPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **XPath** est un type de formulation permettant de parcourir l'arborescence d'un fichier ML rapidement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit l'exemple de XML suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = '''<?xml version=\"1.0\"?>\n",
    "<container>\n",
    "    <data>\n",
    "        <country name=\"Liechtenstein\">\n",
    "            <rank>1</rank>\n",
    "            <year>2008</year>\n",
    "            <gdppc>141100</gdppc>\n",
    "            <neighbor name=\"Austria\" direction=\"E\"/>\n",
    "            <neighbor name=\"Switzerland\" direction=\"W\"/>\n",
    "        </country>\n",
    "        <country name=\"Singapore\">\n",
    "            <rank>4</rank>\n",
    "            <year>2011</year>\n",
    "            <gdppc>59900</gdppc>\n",
    "            <neighbor name=\"Malaysia\" direction=\"N\"/>\n",
    "        </country>\n",
    "        <country name=\"Panama\">\n",
    "            <rank>68</rank>\n",
    "            <year>2011</year>\n",
    "            <gdppc>13600</gdppc>\n",
    "            <neighbor name=\"Costa Rica\" direction=\"W\"/>\n",
    "            <neighbor name=\"Colombia\" direction=\"E\"/>\n",
    "        </country>\n",
    "    </data>\n",
    "</container>'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beautiful Soup** ne fonctionne pas avec les XPath, utilisons le module **xml** de la bibliothèque standard de Python pour voir comment fonctionne ils fonctionnent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as et\n",
    "\n",
    "root = et.fromstring(xml)\n",
    "root.tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chemin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut appliquer un XPath en utilisant la méthode **findall()**. \n",
    "\n",
    "Le XPath suivant récuperera :\n",
    "- l'ensemble des **noeuds** de type ***neighbor*** \n",
    "- qui sont enfants de noeuds ***country*** \n",
    "- qui sont eux-mêmes enfant d'un noeud ***data*** \n",
    "- qui est lui-même un enfant du noeud de départ, que l'on désigne par **__.__** (point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_voisins = root.findall('./data/country/neighbor')\n",
    "liste_voisins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_voisins[0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_voisins[0].attrib['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Rappel :</b> <br>Comme souvent dans un chemin informatique: \n",
    "    <ul>    \n",
    "        <li> le double point ( <b>..</b> ) désigne <b>le parent</b></li> \n",
    "        <li> le point unique ( <b>.</b> ) désigne <b>le noeud lui-même</b></li> \n",
    "        <li> l'asterisk ( <b>*</b> ) désigne <b>n'importe quel nom</b></li> \n",
    "    </ul>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien sûr, si l'on se trompe de **chemin**, cela ne fonctionne pas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall('./country/neighbor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi, alternativement, ne pas demander les **enfants** d'un noeud, mais ses **déscendants**, en utilisant le double slash **//**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le XPath suivant récuperera :\n",
    "- l'ensemble des **noeuds** de type ***neighbor*** \n",
    "- qui sont enfants de noeuds ***country*** \n",
    "- qui sont eux-mêmes **descendants** du noeud de départ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root.findall('.//country/neighbor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le XPath suivant récuperera :\n",
    "- l'ensemble des **noeuds** de type ***neighbor*** \n",
    "- qui sont **descendants** du noeud de départ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall('.//neighbor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attributs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est aussi possible de chercher en utilisant un attribut, on le spécifie alors par \n",
    "\n",
    "```[@attribut=\"valeur\"]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le XPath suivant récuperera :\n",
    "- l'ensemble des **noeuds** de type ***neighbor*** \n",
    "- qui sont **descendants** du noeud de départ\n",
    "- qui ont pour attribut ***name***, la valeur ***Austria***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall('.//neighbor[@name=\"Austria\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le XPath suivant récuperera :\n",
    "- l'ensemble des **noeuds** de type ***year*** \n",
    "- qui sont **descendants** du noeud de départ\n",
    "- qui ont un **parent** qui a pour attribut ***name***, la valeur ***Singapore***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall('.//year/..[@name=\"Singapore\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le XPath suivant récuperera :\n",
    "- l'ensemble des **noeuds** de type ***year***\n",
    "- qui sont **enfants** d'un **parent** qui peut être de **n'importe quel type** et qui a pour attribut ***name***, la valeur ***Singapore***\n",
    "- et qui sont eux-mêmes **descendants** du noeud de départ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall(\".//*[@name='Singapore']/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi rechercher les noeuds par position (attention, en XPath, la première position est 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.findall(\".//neighbor[1]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beautiful Soup** dispose aussi d'outils permettant de faire des choses tout à fait équivalentes, mais en utilisant d'autres moyens (class_, attrs...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Pour aller plus loin :</b> Le <a href=https://www.w3schools.com/xml/xpath_intro.asp>tutorial sur les XPath</a> du site W3Schools</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installer Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lancer **Anaconda prompt**\n",
    "2. Installer le module **scrapy** en exécutant la commande\n",
    "> conda install -y scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du dossier Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T23:15:52.045000Z",
     "start_time": "2019-01-16T23:15:52.037000Z"
    }
   },
   "source": [
    "Créons le dossier Scrapy dans le dossier home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutando el codigo siguiente va a hacer un dossier scrapy en nuestro user (como pybooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "DOSSIER_BASE = pathlib.Path.home() / 'Scrapy'\n",
    "\n",
    "pathlib.Path.home() / 'Scrapy'\n",
    "try:\n",
    "    os.mkdir(DOSSIER_BASE)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "os.chdir(DOSSIER_BASE)\n",
    "\n",
    "os.environ['PATH'] += f';{str(pathlib.Path.home() / \"Anaconda3\" / \"Scripts\")}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qu'un framework ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **framework**, c'est plus qu'un module. C'est un ensemble qui permet d'accélerer grandement le développement d'un programme.\n",
    "\n",
    "Par exemple le framework **django** permet de développer un site web simple en quelques dizaines de lignes de codes.\n",
    "\n",
    "Le framework **scrapy** permet lui de développer un scraper simple en quelques lignes de codes.\n",
    "\n",
    "C'est une **base de programme** pré-organisée, avec :\n",
    "- un grand nombre de fonctions et de **morceaux de programme** pré-écrits (par exemple les boucles du scraper sont déjà écrites) \n",
    "- souvent une **arborescence de dossiers** pré-établie (les sous-dossiers du programme seront crées automatiquement)\n",
    "- souvent aussi un **programme extérieur** à Python qui permet d'exécuter votre programme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le grand avantage est que cela accélère grandement le développement.\n",
    "\n",
    "L'inconvénient principal, c'est que vous êtes moins libre dans le développement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interface de console et IDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy fonctionne sur la base d'un programme extérieur (scrapy.exe), pour lancer les scrapers que vous aller coder, il faudra exécuter ce programme. Pour faire cela, vous avez 2 choix :\n",
    "- utiliser **Anaconda Prompt** et le shell\n",
    "- utiliser un Notebook qui vous servira d'**interface**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En revanche, il est impossible de coder le coeur du scraper sur Notebook, ni de profiter de l'avantage que représente l'exécution interactive.\n",
    "\n",
    "D'ailleurs les fichiers de codes seront des scripts Python (des fichiers **.py**) et pas des fichiers Notebook (**.ipynb**).\n",
    "\n",
    "Il faudra donc aussi utiliser un autre IDE :\n",
    "- Un simple éditeur de texte comme Notepad++ ou l'éditeur de texte interne de Jupyter\n",
    "- Visual Studio Code (VSCode)\n",
    "- Spider\n",
    "- PyCharm community..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le projet Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un **projet** est attaché à un **nom de domaine** (et donc un site).\n",
    "\n",
    "Il faut créer un nouveau projet pour chaque site que vous souhaitez scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer un projet Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOM_PROJET = 'imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject $NOM_PROJET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(DOSSIER_BASE / NOM_PROJET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure d'un projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons la structure de dossiers du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispfolders(folder, level=0):\n",
    "    ELEMENTS_CACHES = ('__pycache__', '__init__.py', '.ipynb_checkpoints')\n",
    "    \n",
    "    print((level > 0)*'|' + level*'--' + folder.split('/')[-1] + '/')\n",
    "    \n",
    "    for elem in os.listdir(folder):  \n",
    "        if elem in ELEMENTS_CACHES:\n",
    "            pass\n",
    "        elif os.path.isdir(f'{folder}/{elem}') and elem != '__pycache__':            \n",
    "            dispfolders(f'{folder}/{elem}', level + 1)\n",
    "        else:\n",
    "            print('|' + (level + 1)*'--' + elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispfolders('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un dossier du nom du projet, celui-ci contient 2 éléments :\n",
    "- un fichier **scrapy.cfg** : il contient 2-3 paramètres généraux sur le projet\n",
    "- un dossier du même nom que le projet qui contient :\n",
    "    - un fichier **items.py** : le fichier permettant de configurer la structure des données en sortie\n",
    "    - un fichier **settings.py** : le fichier permettant de vraiment paramètrer le projet\n",
    "    - un fichier **middlewares.py** : le fichier permettant de coder des Middlewares (avancé)\n",
    "    - un fichier **pipelines.py** : le fichier permettant de paramètrer la séquence chronologique des scrapers (avancé)\n",
    "    - un dossier **spiders** : le dossier qui contiendra des unités de scraping individuelles (les spiders)\n",
    "- nous ajouterons dans le dossier de base nos fichiers de log et de données en sortie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paramétrer le projet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite il faut paramétrer le projet, on va aller mettre les valeurs suivantes dans le fichier **settings.py** :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:64.0) Gecko/20100101 Firefox/64.0'\n",
    "- ROBOTSTXT_OBEY = True\n",
    "- CONCURRENT_REQUESTS = 16\n",
    "- DOWNLOAD_DELAY = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une **spider** est un scraper. \n",
    "\n",
    "On peut en créer plusieurs pour un même site (et donc dans un même projet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'une spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy genspider top251 www.imdb.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispfolders('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un nouveau fichier Python du nom de la spider a été créé dans le sous-dossier **spiders**.\n",
    "\n",
    "C'est ce fichier qui contiendra le code de notre scraper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codage d'une spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant toute chose, il faut aller modifier l'attribut **start_urls** en indiquant l'adresse exacte servant de point de départ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_urls = ['https://www.imdb.com/chart/top/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode spider.parse() va être exécutée au démarrage de la spider, c'est le point de départ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    urls = response.xpath('//tbody[@class=\"lister-list\"]/tr/td[@class=\"titleColumn\"]/a')\n",
    "\n",
    "    for url in urls:\n",
    "        yield response.follow(url, callback=self.parse_page_film)\n",
    "        break # cale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette méthode va appeler tour à tour une autre méthode, parse_page_film() qui sera la méthode permettant de traiter une unique page web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_film(self, response):\n",
    "    item = ImdbItem()\n",
    "    \n",
    "    item['url'] = response.url\n",
    "    subtext = response.xpath('//div[@class=\"subtext\"]/a/text()').extract()        \n",
    "    item['genres'] = subtext[0].strip()\n",
    "    \n",
    "    yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais pour ça il faut au préalable définit l'objet ImdbItem dont la définition est contenue dans le fichier **items.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class ImdbItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    url = scrapy.Field()\n",
    "    genres = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test de la spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove('log.log')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.remove('test.csv')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "print('Exécution du crawler...')\n",
    "print()\n",
    "\n",
    "! scrapy crawl top250 --logfile=log.log -o test.csv\n",
    "\n",
    "print()\n",
    "print('Fin de l\\'exécution.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploitation les données en sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Pour aller plus loin :</b> <a href=https://doc.scrapy.org/en/latest/intro/tutorial.html> Tutorial du site officiel de Scrapy </a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Selenium** est un **émulateur de navigateur web**. Il va passer par un navigateur pour effectuer les actions souhaitées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation du module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lancer **Anaconda prompt**\n",
    "2. Installer le module **selenium** en exécutant la commande\n",
    "> conda install -c conda-forge -y selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite charger les modules de Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.webdriver as selweb\n",
    "import selenium.common as selcom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les webdrivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut mettre les fichiers exécutables dans {dossier Home}/Anaconda3/Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Firefox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mozilla/geckodriver/releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    selweb.Firefox()\n",
    "except selcom.exceptions.WebDriverException:\n",
    "    print('Le driver Firefox n\\'est pas installé')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T00:34:54.176000Z",
     "start_time": "2019-01-17T00:34:54.169000Z"
    }
   },
   "source": [
    "https://sites.google.com/a/chromium.org/chromedriver/downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    selweb.Chrome()\n",
    "except selcom.exceptions.WebDriverException:\n",
    "    print('Le driver Chrome n\\'est pas installé')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premiers essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = selweb.Firefox()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**browser** nous servira de **poignée** (handle en anglais) pour interagir avec la fenêtre qui s'est ouverte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.get('https://duckduckgo.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_form = browser.find_element_by_id('search_form_input_homepage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_form.send_keys('Python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_form.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = browser.find_elements_by_class_name('result')\n",
    "premier_resultat = resultats[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premier_resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le headless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On parle de **headless** pour désigner l'exécution du navigateur sans fenêtre. C'est plus pratique pour l'automatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = selweb.firefox.options.Options()\n",
    "opts.headless = True\n",
    "\n",
    "browser = selweb.Firefox(options=opts)\n",
    "\n",
    "browser.get('https://duckduckgo.com')\n",
    "\n",
    "search_form = browser.find_element_by_id('search_form_input_homepage')\n",
    "search_form.send_keys('Python')\n",
    "\n",
    "search_form.submit()\n",
    "time.sleep(1)\n",
    "\n",
    "resultats = browser.find_elements_by_class_name('result')\n",
    "premier_resultat = resultats[0].text\n",
    "\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "premier_resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper grâce à Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = selweb.Firefox()\n",
    "browser.get('https://www.pagesjaunes.fr/recherche/creteil-94/epicerie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = browser.find_elements_by_xpath('//a[@class=\"denomination-links pj-link\"]')\n",
    "adresses = browser.find_elements_by_xpath('//a[@class=\"adresse pj-lb pj-link\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir le contenu texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adresses[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir la valeur d'un attribut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adresses[0].get_attribute('data-pjstats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour obtenir le code de la balise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adresses[0].get_attribute('outerHTML')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite traiter et enregistrer les données avec les moyens habituels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_couples = [(n.text, a.text) for n, a in zip(names, adresses)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(liste_couples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Pour aller plus loin :</b> <a href=https://selenium-python.readthedocs.io/> Doc officielle de Selenium </a></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "500px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "600px",
    "left": "0px",
    "right": "1359px",
    "top": "107px",
    "width": "180px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
